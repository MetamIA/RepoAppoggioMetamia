# -*- coding: utf-8 -*-
"""sentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jlIba7c04EThT7c8_d9Ifo9b6-9xcbKu
"""

# Google Trends Analyzer per l'Italia
# Programma per analizzare i trend di ricerca in Italia con suddivisione per regioni

# Installazione delle librerie necessarie
!pip install pytrends pandas matplotlib seaborn

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pytrends.request import TrendReq
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# Configurazione per i grafici
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

class GoogleTrendsItaly:
    def __init__(self):
        """Inizializza il client per Google Trends"""
        self.pytrends = TrendReq(hl='it-IT', tz=360)

        # Codici delle regioni italiane per Google Trends
        self.regioni_italiane = {
            'IT-21': 'Piemonte',
            'IT-23': 'Valle d\'Aosta',
            'IT-25': 'Lombardia',
            'IT-32': 'Trentino-Alto Adige',
            'IT-34': 'Veneto',
            'IT-36': 'Friuli-Venezia Giulia',
            'IT-42': 'Liguria',
            'IT-45': 'Emilia-Romagna',
            'IT-52': 'Toscana',
            'IT-55': 'Umbria',
            'IT-57': 'Marche',
            'IT-62': 'Lazio',
            'IT-65': 'Abruzzo',
            'IT-67': 'Molise',
            'IT-72': 'Campania',
            'IT-75': 'Puglia',
            'IT-77': 'Basilicata',
            'IT-78': 'Calabria',
            'IT-82': 'Sicilia',
            'IT-88': 'Sardegna'
        }

    def analizza_trend(self, parola_chiave, periodo='today 12-m', mostra_regioni=True):
        """
        Analizza il trend di una parola chiave per l'Italia

        Args:
            parola_chiave (str): La parola da analizzare
            periodo (str): Periodo di analisi (es: 'today 12-m', 'today 3-m', '2023-01-01 2023-12-31')
            mostra_regioni (bool): Se mostrare anche i dati per regioni
        """

        print(f"üîç Analizzando il trend per: '{parola_chiave}'")
        print(f"üìÖ Periodo: {periodo}")
        print(f"üáÆüáπ Paese: Italia")
        print("-" * 50)

        try:
            # Analisi trend temporale per l'Italia
            self.pytrends.build_payload([parola_chiave], cat=0, timeframe=periodo, geo='IT')

            # Ottieni dati nel tempo
            trend_nel_tempo = self.pytrends.interest_over_time()

            if trend_nel_tempo.empty:
                print("‚ùå Nessun dato trovato per questa parola chiave.")
                return

            # Rimuovi la colonna 'isPartial' se presente
            if 'isPartial' in trend_nel_tempo.columns:
                trend_nel_tempo = trend_nel_tempo.drop('isPartial', axis=1)

            # Visualizza trend nel tempo
            plt.figure(figsize=(15, 10))

            # Grafico principale del trend
            plt.subplot(2, 2, 1)
            plt.plot(trend_nel_tempo.index, trend_nel_tempo[parola_chiave],
                    linewidth=2, marker='o', markersize=4)
            plt.title(f'Trend di "{parola_chiave}" in Italia', fontsize=14, fontweight='bold')
            plt.xlabel('Data')
            plt.ylabel('Interesse relativo')
            plt.xticks(rotation=45)
            plt.grid(True, alpha=0.3)

            # Statistiche base
            max_interesse = trend_nel_tempo[parola_chiave].max()
            min_interesse = trend_nel_tempo[parola_chiave].min()
            media_interesse = trend_nel_tempo[parola_chiave].mean()

            plt.text(0.02, 0.98, f'Max: {max_interesse}\nMin: {min_interesse}\nMedia: {media_interesse:.1f}',
                    transform=plt.gca().transAxes, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

            # Analisi per regioni se richiesta
            if mostra_regioni:
                try:
                    # Ottieni dati per regioni
                    trend_regioni = self.pytrends.interest_by_region(resolution='REGION')

                    if not trend_regioni.empty:
                        # Ordina per interesse decrescente
                        trend_regioni = trend_regioni.sort_values(by=parola_chiave, ascending=False)

                        # Top 10 regioni
                        top_regioni = trend_regioni.head(10)

                        # Grafico a barre per regioni
                        plt.subplot(2, 2, 2)
                        bars = plt.bar(range(len(top_regioni)), top_regioni[parola_chiave])
                        plt.title(f'Top 10 Regioni per "{parola_chiave}"', fontsize=14, fontweight='bold')
                        plt.xlabel('Regioni')
                        plt.ylabel('Interesse relativo')
                        plt.xticks(range(len(top_regioni)), top_regioni.index, rotation=45)

                        # Colori gradient per le barre
                        colors = plt.cm.viridis(np.linspace(0, 1, len(top_regioni)))
                        for bar, color in zip(bars, colors):
                            bar.set_color(color)

                        # Heatmap delle regioni
                        plt.subplot(2, 2, 3)
                        # Crea una matrice per la heatmap
                        regioni_matrix = trend_regioni.head(15).values.reshape(-1, 1)
                        sns.heatmap(regioni_matrix,
                                   yticklabels=trend_regioni.head(15).index,
                                   xticklabels=[parola_chiave],
                                   annot=True, fmt='.0f', cmap='YlOrRd')
                        plt.title('Heatmap Interesse per Regioni', fontsize=14, fontweight='bold')

                        # Distribuzione dell'interesse
                        plt.subplot(2, 2, 4)
                        plt.hist(trend_regioni[parola_chiave], bins=10, alpha=0.7, edgecolor='black')
                        plt.title('Distribuzione Interesse tra Regioni', fontsize=14, fontweight='bold')
                        plt.xlabel('Interesse relativo')
                        plt.ylabel('Numero di regioni')
                        plt.grid(True, alpha=0.3)

                except Exception as e:
                    print(f"‚ö†Ô∏è Errore nell'analisi regionale: {e}")

            plt.tight_layout()
            plt.show()

            # Stampa risultati testuali
            print("\nüìä RISULTATI ANALISI:")
            print(f"‚Ä¢ Interesse massimo: {max_interesse} (il {trend_nel_tempo[parola_chiave].idxmax().strftime('%d/%m/%Y')})")
            print(f"‚Ä¢ Interesse minimo: {min_interesse}")
            print(f"‚Ä¢ Interesse medio: {media_interesse:.1f}")

            if mostra_regioni and not trend_regioni.empty:
                print(f"\nüèÜ TOP 5 REGIONI:")
                for i, (regione, valore) in enumerate(trend_regioni.head(5).iterrows(), 1):
                    print(f"{i}. {regione}: {valore[parola_chiave]}")

            # Ricerche correlate
            try:
                correlate = self.pytrends.related_queries()
                if correlate[parola_chiave]['top'] is not None:
                    print(f"\nüîó RICERCHE CORRELATE:")
                    for i, query in enumerate(correlate[parola_chiave]['top'].head(5)['query'], 1):
                        print(f"{i}. {query}")
            except:
                pass

            return trend_nel_tempo, trend_regioni if mostra_regioni else None

        except Exception as e:
            print(f"‚ùå Errore nell'analisi: {e}")
            return None, None

    def confronta_parole(self, parole_chiave, periodo='today 12-m'):
        """
        Confronta pi√π parole chiave

        Args:
            parole_chiave (list): Lista di parole da confrontare (max 5)
            periodo (str): Periodo di analisi
        """

        if len(parole_chiave) > 5:
            print("‚ö†Ô∏è Massimo 5 parole chiave per confronto")
            parole_chiave = parole_chiave[:5]

        print(f"üîç Confrontando: {', '.join(parole_chiave)}")
        print(f"üìÖ Periodo: {periodo}")
        print("-" * 50)

        try:
            self.pytrends.build_payload(parole_chiave, cat=0, timeframe=periodo, geo='IT')
            trend_confronto = self.pytrends.interest_over_time()

            if trend_confronto.empty:
                print("‚ùå Nessun dato trovato.")
                return

            # Rimuovi colonna isPartial
            if 'isPartial' in trend_confronto.columns:
                trend_confronto = trend_confronto.drop('isPartial', axis=1)

            # Grafico di confronto
            plt.figure(figsize=(15, 8))

            for parola in parole_chiave:
                plt.plot(trend_confronto.index, trend_confronto[parola],
                        linewidth=2, marker='o', markersize=4, label=parola)

            plt.title('Confronto Trend delle Parole Chiave', fontsize=16, fontweight='bold')
            plt.xlabel('Data')
            plt.ylabel('Interesse relativo')
            plt.legend()
            plt.xticks(rotation=45)
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.show()

            # Statistiche comparative
            print("\nüìä STATISTICHE COMPARATIVE:")
            for parola in parole_chiave:
                media = trend_confronto[parola].mean()
                massimo = trend_confronto[parola].max()
                print(f"‚Ä¢ {parola}: Media={media:.1f}, Massimo={massimo}")

            return trend_confronto

        except Exception as e:
            print(f"‚ùå Errore nel confronto: {e}")
            return None

# Importa numpy per i colori
import numpy as np

# Inizializza l'analyzer
analyzer = GoogleTrendsItaly()

# ESEMPI DI UTILIZZO:

print("üöÄ GOOGLE TRENDS ANALYZER PER L'ITALIA")
print("=" * 50)

# Esempio 1: Analisi singola parola
print("\n1Ô∏è‚É£ ESEMPIO: Analisi trend 'pizza'")
trend_dati, regioni_dati = analyzer.analizza_trend('pizza', periodo='today 12-m')

# Esempio 2: Confronto tra parole
print("\n2Ô∏è‚É£ ESEMPIO: Confronto 'pizza' vs 'pasta' vs 'gelato'")
confronto = analyzer.confronta_parole(['pizza', 'pasta', 'gelato'], periodo='today 6-m')

# INTERFACCIA INTERATTIVA
print("\n" + "="*60)
print("üéØ INTERFACCIA INTERATTIVA")
print("="*60)

def analisi_interattiva():
    """Funzione per l'analisi interattiva"""

    print("\nInserisci i parametri per l'analisi:")

    # Input parola chiave
    parola = input("üîç Parola chiave da analizzare: ").strip()

    if not parola:
        print("‚ùå Parola chiave non valida.")
        return

    # Input periodo
    print("\nScegli il periodo:")
    print("1. Ultimi 12 mesi (today 12-m)")
    print("2. Ultimi 3 mesi (today 3-m)")
    print("3. Ultimo mese (today 1-m)")
    print("4. Ultimi 5 anni (today 5-y)")
    print("5. Periodo personalizzato (formato: YYYY-MM-DD YYYY-MM-DD)")

    scelta_periodo = input("Inserisci la scelta (1-5): ").strip()

    periodi = {
        '1': 'today 12-m',
        '2': 'today 3-m',
        '3': 'today 1-m',
        '4': 'today 5-y'
    }

    if scelta_periodo in periodi:
        periodo = periodi[scelta_periodo]
    elif scelta_periodo == '5':
        periodo = input("Inserisci il periodo (es: 2023-01-01 2023-12-31): ").strip()
    else:
        print("‚ö†Ô∏è Scelta non valida, uso periodo predefinito (12 mesi)")
        periodo = 'today 12-m'

    # Input analisi regionale
    regioni = input("üèõÔ∏è Vuoi l'analisi per regioni? (s/n): ").strip().lower()
    mostra_regioni = regioni in ['s', 'si', 's√¨', 'y', 'yes']

    # Esegui analisi
    print(f"\nüöÄ Avvio analisi per '{parola}'...")
    analyzer.analizza_trend(parola, periodo, mostra_regioni)

# Avvia l'interfaccia interattiva
print("\nüéÆ Pronto per l'analisi interattiva!")
print("Esegui la cella seguente per iniziare:")
print("analisi_interattiva()")

# FUNZIONI AGGIUNTIVE UTILI

def periodi_disponibili():
    """Mostra i formati di periodo disponibili"""
    print("üìÖ FORMATI PERIODO DISPONIBILI:")
    print("‚Ä¢ 'today 1-m' - Ultimo mese")
    print("‚Ä¢ 'today 3-m' - Ultimi 3 mesi")
    print("‚Ä¢ 'today 12-m' - Ultimi 12 mesi")
    print("‚Ä¢ 'today 5-y' - Ultimi 5 anni")
    print("‚Ä¢ 'YYYY-MM-DD YYYY-MM-DD' - Periodo personalizzato")
    print("‚Ä¢ 'today 1-d' - Ultimo giorno (solo per trend molto recenti)")
    print("‚Ä¢ 'today 7-d' - Ultima settimana")

def suggerimenti_parole():
    """Suggerisce alcune parole chiave interessanti per l'Italia"""
    print("üí° SUGGERIMENTI PAROLE CHIAVE INTERESSANTI:")
    print("üçï Cibo: pizza, pasta, gelato, tiramis√π, carbonara")
    print("üèñÔ∏è Turismo: vacanze, mare, montagna, Roma, Venezia")
    print("‚öΩ Sport: calcio, Serie A, Juventus, Milan, Inter")
    print("üé≠ Cultura: opera, arte, musei, cinema, festival")
    print("üì± Tecnologia: smartphone, iPhone, Android, TikTok")
    print("üèõÔ∏è Politica: elezioni, governo, parlamento")
    print("üå°Ô∏è Stagionalit√†: estate, inverno, Natale, Pasqua")

# Mostra informazioni utili
periodi_disponibili()
print()
suggerimenti_parole()

print("\n" + "="*60)
print("‚úÖ SETUP COMPLETATO!")
print("="*60)
print("üí° Per iniziare, esegui: analisi_interattiva()")
print("üîç Oppure usa direttamente: analyzer.analizza_trend('tua_parola')")
print("‚öñÔ∏è Per confronti: analyzer.confronta_parole(['parola1', 'parola2'])")

analisi_interattiva()

# Estrattore Intelligente di Oggetti con Dependency Parsing e Analisi Semantica
# Identifica automaticamente gli oggetti rilevanti usando struttura sintattica e semantica

# Installazione delle librerie necessarie
!pip install nltk spacy textblob
!python -m spacy download it_core_news_sm
!python -m spacy download en_core_web_sm

import nltk
import spacy
from textblob import TextBlob
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter, defaultdict
import re
from nltk.corpus import wordnet as wn
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import WordNetLemmatizer
import warnings
warnings.filterwarnings('ignore')

# Download dei dati NLTK necessari
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('omw-1.4')

class SmartObjectExtractor:
    def __init__(self):
        """Inizializza l'estrattore intelligente di oggetti"""
        print("üß† Inizializzazione Estrattore Intelligente di Oggetti...")

        # Carica modelli di lingua
        try:
            self.nlp_it = spacy.load("it_core_news_sm")
            print("‚úÖ Modello italiano caricato")
        except:
            print("‚ö†Ô∏è Modello italiano non disponibile")
            self.nlp_it = None

        try:
            self.nlp_en = spacy.load("en_core_web_sm")
            print("‚úÖ Modello inglese caricato")
        except:
            print("‚ö†Ô∏è Modello inglese non disponibile")
            self.nlp_en = None

        # Inizializza lemmatizer per WordNet
        self.lemmatizer = WordNetLemmatizer()

        # Solo stopwords base (il resto lo determiniamo semanticamente)
        self.stop_words_it = set(stopwords.words('italian'))
        self.stop_words_en = set(stopwords.words('english'))

        # Dependency labels che indicano oggetti importanti
        self.important_deps = {
            'nsubj',      # soggetto nominale
            'dobj',       # oggetto diretto
            'pobj',       # oggetto di preposizione
            'nsubjpass',  # soggetto passivo
            'compound',   # sostantivo composto
            'appos',      # apposizione
            'conj'        # congiunzione (oggetti in lista)
        }

        # Dependency labels che indicano parole meno rilevanti
        self.weak_deps = {
            'det',        # determinanti
            'aux',        # ausiliari
            'cop',        # copula
            'mark',       # marcatori subordinanti
            'punct',      # punteggiatura
            'cc'          # congiunzioni coordinanti
        }

        print("‚úÖ Estrattore intelligente inizializzato!")

    def detect_language(self, text):
        """Rileva la lingua del testo"""
        try:
            blob = TextBlob(text)
            lang = blob.detect_language()
            return 'it' if lang in ['it', 'la'] else 'en'
        except:
            # Fallback basato su parole italiane comuni
            italian_indicators = ['il', 'la', 'di', 'che', 'e', 'a', 'un', 'per', 'con', 'da', 'in', 'del', 'alla', '√®']
            words = text.lower().split()
            italian_count = sum(1 for word in words if word in italian_indicators)
            return 'it' if italian_count > len(words) * 0.08 else 'en'

    def analyze_semantic_importance(self, token, language='it'):
        """Analizza l'importanza semantica di un token usando WordNet"""

        if not token.text.isalpha() or len(token.text) < 3:
            return 0.0

        # Cerca in WordNet
        synsets = wn.synsets(token.lemma_.lower())

        if not synsets:
            return 0.5  # Score neutro se non trovato

        importance_score = 0.0

        for synset in synsets:
            # 1. Concretezza: sostantivi concreti sono pi√π importanti
            if synset.pos() == 'n':  # sostantivo
                # Cerca iponimi (pi√π specifico = pi√π concreto)
                hyponyms = synset.hyponyms()
                if hyponyms:
                    importance_score += 0.3 * len(hyponyms[:5])  # Max bonus da iponimi

                # Cerca iperonymi (concetti generali sono meno importanti)
                hypernyms = synset.hypernyms()
                if len(hypernyms) > 3:  # Troppo astratto
                    importance_score -= 0.2

                # 2. Specificit√†: definizioni lunghe = concetti pi√π specifici
                definition_length = len(synset.definition().split())
                if definition_length > 10:
                    importance_score += 0.4
                elif definition_length < 5:
                    importance_score -= 0.2

                # 3. Esempi: presenza di esempi indica concretezza
                if synset.examples():
                    importance_score += 0.3

        return max(0.0, min(2.0, importance_score))  # Normalizza tra 0 e 2

    def analyze_syntactic_role(self, token):
        """Analizza il ruolo sintattico del token nell'albero delle dipendenze"""

        role_score = 0.0

        # 1. Dependency label principale
        if token.dep_ in self.important_deps:
            role_score += 2.0
        elif token.dep_ in self.weak_deps:
            role_score -= 1.0
        else:
            role_score += 0.5  # Score neutro per altri dep

        # 2. Posizione nella frase (inizio = pi√π importante)
        sentence_position = token.i / len(token.doc)
        if sentence_position < 0.3:  # Prime 30% della frase
            role_score += 0.5
        elif sentence_position > 0.8:  # Ultime 20% della frase
            role_score -= 0.3

        # 3. Presenza di modificatori (indica importanza)
        modifiers = list(token.children)
        if modifiers:
            # Aggettivi, avverbi, altre specifiche
            modifier_bonus = len([child for child in modifiers if child.pos_ in ['ADJ', 'ADV']]) * 0.3
            role_score += modifier_bonus

        # 4. Dipendenza dal verbo principale
        if token.head.pos_ == 'VERB' and token.dep_ in ['nsubj', 'dobj']:
            role_score += 1.0  # Soggetto/oggetto di verbo principale

        # 5. Lunghezza token (troppo corto spesso non significativo)
        if len(token.text) < 3:
            role_score -= 0.5
        elif len(token.text) > 8:
            role_score += 0.3  # Parole lunghe spesso pi√π specifiche

        return role_score

    def extract_objects_with_context(self, text, language='it', top_k=15):
        """Estrae oggetti usando dependency parsing e analisi semantica"""

        print(f"üß† Analisi intelligente oggetti (lingua: {language})")

        # Seleziona modello
        nlp = self.nlp_it if language == 'it' else self.nlp_en

        if not nlp:
            print("‚ùå Modello spaCy non disponibile")
            return []

        # Processa il testo
        doc = nlp(text)

        object_candidates = []

        # Analizza ogni token
        for token in doc:
            if token.pos_ in ['NOUN', 'PROPN'] and not token.is_stop:

                # 1. Score sintattico (dependency parsing)
                syntactic_score = self.analyze_syntactic_role(token)

                # 2. Score semantico (WordNet)
                semantic_score = self.analyze_semantic_importance(token, language)

                # 3. Score di contesto (entit√† nominate)
                context_score = 0.0
                for ent in doc.ents:
                    if token.text in ent.text:
                        context_score += 1.0  # Parte di entit√† nominata
                        break

                # 4. Score di frequenza nella frase
                frequency_score = len([t for t in doc if t.lemma_ == token.lemma_]) * 0.2

                # Score finale combinato
                final_score = syntactic_score + semantic_score + context_score + frequency_score

                object_candidates.append({
                    'text': token.lemma_.lower(),
                    'original': token.text,
                    'pos': token.pos_,
                    'dep': token.dep_,
                    'syntactic_score': syntactic_score,
                    'semantic_score': semantic_score,
                    'context_score': context_score,
                    'frequency_score': frequency_score,
                    'final_score': final_score,
                    'sentence_index': token.sent.start
                })

        # Raggruppa oggetti simili e somma scores
        grouped_objects = defaultdict(lambda: {
            'total_score': 0.0,
            'count': 0,
            'details': [],
            'best_form': ''
        })

        for obj in object_candidates:
            key = obj['text']
            grouped_objects[key]['total_score'] += obj['final_score']
            grouped_objects[key]['count'] += 1
            grouped_objects[key]['details'].append(obj)

            # Mantieni la forma migliore (pi√π frequente o con score pi√π alto)
            if not grouped_objects[key]['best_form'] or obj['final_score'] > max(d['final_score'] for d in grouped_objects[key]['details'][:-1]):
                grouped_objects[key]['best_form'] = obj['original']

        # Calcola score medio e applica bonus per frequenza
        final_objects = []
        for text, data in grouped_objects.items():
            avg_score = data['total_score'] / data['count']
            frequency_bonus = min(data['count'] * 0.3, 1.0)  # Max bonus 1.0

            final_score = avg_score + frequency_bonus

            final_objects.append({
                'object': data['best_form'],
                'lemma': text,
                'score': final_score,
                'frequency': data['count'],
                'avg_syntactic': sum(d['syntactic_score'] for d in data['details']) / len(data['details']),
                'avg_semantic': sum(d['semantic_score'] for d in data['details']) / len(data['details']),
                'contexts': list(set(d['dep'] for d in data['details']))
            })

        # Ordina per score finale
        final_objects.sort(key=lambda x: x['score'], reverse=True)

        return final_objects[:top_k]

    def extract_compound_objects(self, text, language='it'):
        """Estrae oggetti composti usando dependency parsing"""

        print("üîó Estrazione oggetti composti...")

        nlp = self.nlp_it if language == 'it' else self.nlp_en

        if not nlp:
            return []

        doc = nlp(text)
        compounds = []

        for token in doc:
            if token.pos_ in ['NOUN', 'PROPN']:
                compound_parts = [token.text.lower()]

                # Cerca modificatori che formano compounds
                for child in token.children:
                    if child.dep_ in ['compound', 'amod'] and child.pos_ in ['NOUN', 'PROPN', 'ADJ']:
                        compound_parts.insert(0, child.text.lower())
                    elif child.dep_ == 'prep':
                        # Gestisce costruzioni come "carta di credito"
                        for prep_child in child.children:
                            if prep_child.dep_ == 'pobj':
                                compound_phrase = f"{token.text.lower()} {child.text} {prep_child.text.lower()}"

                                # Valuta se √® un compound semanticamente significativo
                                compound_score = 0.0

                                # Controlla in WordNet se il compound esiste
                                compound_synsets = wn.synsets(compound_phrase.replace(' ', '_'))
                                if compound_synsets:
                                    compound_score += 2.0

                                # Controlla parti individuali
                                part_scores = []
                                for part in [token.text.lower(), prep_child.text.lower()]:
                                    part_synsets = wn.synsets(part)
                                    if part_synsets:
                                        part_scores.append(len(part_synsets))

                                if part_scores:
                                    compound_score += sum(part_scores) * 0.1

                                if compound_score > 0.5:
                                    compounds.append((compound_phrase, compound_score))

                # Crea compound da modificatori adiacenti
                if len(compound_parts) > 1:
                    compound_text = ' '.join(compound_parts)

                    # Valuta compound
                    compound_score = len(compound_parts) * 0.5

                    # Bonus se trovato in WordNet
                    compound_synsets = wn.synsets(compound_text.replace(' ', '_'))
                    if compound_synsets:
                        compound_score += 1.0

                    compounds.append((compound_text, compound_score))

        # Rimuovi duplicati e ordina per score
        unique_compounds = {}
        for compound, score in compounds:
            if compound not in unique_compounds or score > unique_compounds[compound]:
                unique_compounds[compound] = score

        sorted_compounds = sorted(unique_compounds.items(), key=lambda x: x[1], reverse=True)

        return sorted_compounds[:10]

    def analyze_object_relationships(self, text, language='it'):
        """Analizza le relazioni tra oggetti nel testo"""

        print("üîç Analisi relazioni tra oggetti...")

        nlp = self.nlp_it if language == 'it' else self.nlp_en

        if not nlp:
            return {}

        doc = nlp(text)
        relationships = defaultdict(list)

        # Trova relazioni verbo-oggetto
        for token in doc:
            if token.pos_ == 'VERB':
                subjects = []
                objects = []

                for child in token.children:
                    if child.dep_ in ['nsubj', 'nsubjpass'] and child.pos_ in ['NOUN', 'PROPN']:
                        subjects.append(child.lemma_.lower())
                    elif child.dep_ in ['dobj', 'pobj'] and child.pos_ in ['NOUN', 'PROPN']:
                        objects.append(child.lemma_.lower())

                # Registra relazioni
                verb = token.lemma_.lower()
                for subj in subjects:
                    for obj in objects:
                        relationships[subj].append(f"{verb} ‚Üí {obj}")
                        relationships[obj].append(f"‚Üê {verb} {subj}")

        return dict(relationships)

    def visualize_dependency_tree(self, text, language='it'):
        """Visualizza l'albero delle dipendenze per debug"""

        nlp = self.nlp_it if language == 'it' else self.nlp_en

        if not nlp:
            print("‚ùå Modello spaCy non disponibile")
            return

        doc = nlp(text)

        print("üå≥ ALBERO DELLE DIPENDENZE:")
        print("=" * 50)

        for token in doc:
            if token.pos_ in ['NOUN', 'PROPN', 'VERB']:
                indent = "  " * (token.depth if hasattr(token, 'depth') else 0)
                children_info = [f"{child.text}({child.dep_})" for child in token.children]
                children_str = f" ‚Üí [{', '.join(children_info)}]" if children_info else ""

                print(f"{indent}{token.text} ({token.pos_}, {token.dep_}){children_str}")

    def extract_comprehensive(self, text, top_k=10, show_analysis=True):
        """Estrazione completa con analisi dettagliata"""

        print("üéØ ESTRAZIONE COMPLETA OGGETTI")
        print("=" * 50)
        print(f"üìù Testo: {text[:100]}{'...' if len(text) > 100 else ''}")

        # Rileva lingua
        language = self.detect_language(text)
        print(f"üåç Lingua rilevata: {'Italiano' if language == 'it' else 'Inglese'}")
        print("-" * 50)

        # Mostra albero dipendenze se richiesto
        if show_analysis:
            self.visualize_dependency_tree(text, language)
            print()

        # Estrai oggetti semplici
        simple_objects = self.extract_objects_with_context(text, language, top_k)

        # Estrai oggetti composti
        compound_objects = self.extract_compound_objects(text, language)

        # Analizza relazioni
        relationships = self.analyze_object_relationships(text, language)

        # Visualizza risultati
        self.visualize_results(simple_objects, compound_objects, relationships, text)

        return simple_objects, compound_objects, relationships

    def visualize_results(self, simple_objects, compound_objects, relationships, text):
        """Visualizza i risultati dell'estrazione"""

        # Configura subplot
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Analisi Intelligente degli Oggetti', fontsize=16, fontweight='bold')

        # 1. Oggetti semplici - scores
        if simple_objects:
            objects = [obj['object'] for obj in simple_objects[:10]]
            scores = [obj['score'] for obj in simple_objects[:10]]

            bars = axes[0, 0].bar(range(len(objects)), scores, color='skyblue')
            axes[0, 0].set_title('Top Oggetti per Score', fontweight='bold')
            axes[0, 0].set_xlabel('Oggetti')
            axes[0, 0].set_ylabel('Score')
            axes[0, 0].set_xticks(range(len(objects)))
            axes[0, 0].set_xticklabels(objects, rotation=45, ha='right')

            # Aggiungi valori sulle barre
            for bar, score in zip(bars, scores):
                axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                               f'{score:.2f}', ha='center', va='bottom', fontsize=9)

        # 2. Breakdown scores (sintattico vs semantico)
        if simple_objects:
            syntactic_scores = [obj['avg_syntactic'] for obj in simple_objects[:8]]
            semantic_scores = [obj['avg_semantic'] for obj in simple_objects[:8]]
            labels = [obj['object'][:8] for obj in simple_objects[:8]]

            x = range(len(labels))
            width = 0.35

            axes[0, 1].bar([i - width/2 for i in x], syntactic_scores, width, label='Sintattico', color='orange')
            axes[0, 1].bar([i + width/2 for i in x], semantic_scores, width, label='Semantico', color='green')

            axes[0, 1].set_title('Score Sintattico vs Semantico', fontweight='bold')
            axes[0, 1].set_xlabel('Oggetti')
            axes[0, 1].set_ylabel('Score')
            axes[0, 1].set_xticks(x)
            axes[0, 1].set_xticklabels(labels, rotation=45, ha='right')
            axes[0, 1].legend()

        # 3. Frequenza oggetti
        if simple_objects:
            frequencies = [obj['frequency'] for obj in simple_objects[:10]]
            axes[0, 2].hist(frequencies, bins=5, alpha=0.7, color='lightgreen', edgecolor='black')
            axes[0, 2].set_title('Distribuzione Frequenze', fontweight='bold')
            axes[0, 2].set_xlabel('Frequenza nel testo')
            axes[0, 2].set_ylabel('Numero oggetti')

        # 4. Oggetti composti
        if compound_objects:
            comp_names = [comp[:15] for comp, _ in compound_objects[:8]]
            comp_scores = [score for _, score in compound_objects[:8]]

            axes[1, 0].bar(range(len(comp_names)), comp_scores, color='coral')
            axes[1, 0].set_title('Oggetti Composti', fontweight='bold')
            axes[1, 0].set_xlabel('Oggetti Composti')
            axes[1, 0].set_ylabel('Score')
            axes[1, 0].set_xticks(range(len(comp_names)))
            axes[1, 0].set_xticklabels(comp_names, rotation=45, ha='right')

        # 5. Dependency labels distribution
        if simple_objects:
            all_contexts = []
            for obj in simple_objects:
                all_contexts.extend(obj['contexts'])

            context_counts = Counter(all_contexts)

            if context_counts:
                contexts = list(context_counts.keys())[:6]
                counts = [context_counts[ctx] for ctx in contexts]

                axes[1, 1].pie(counts, labels=contexts, autopct='%1.1f%%')
                axes[1, 1].set_title('Ruoli Sintattici', fontweight='bold')

        # 6. Statistiche e top oggetti
        word_count = len(text.split())
        unique_objects = len(set(obj['lemma'] for obj in simple_objects))

        stats_text = f"""
        üìä STATISTICHE:
        ‚Ä¢ Parole totali: {word_count}
        ‚Ä¢ Oggetti unici: {unique_objects}
        ‚Ä¢ Oggetti composti: {len(compound_objects)}
        ‚Ä¢ Con relazioni: {len(relationships)}

        üèÜ TOP 5 OGGETTI:
        """

        for i, obj in enumerate(simple_objects[:5], 1):
            stats_text += f"\n{i}. {obj['object']} ({obj['score']:.2f})"

        axes[1, 2].text(0.1, 0.9, stats_text, transform=axes[1, 2].transAxes,
                        verticalalignment='top', fontsize=10,
                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        axes[1, 2].set_title('Statistiche', fontweight='bold')
        axes[1, 2].axis('off')

        plt.tight_layout()
        plt.show()

        # Stampa risultati testuali dettagliati
        print("\nüéØ OGGETTI PRINCIPALI:")
        print("=" * 60)

        for i, obj in enumerate(simple_objects[:8], 1):
            print(f"{i:2d}. {obj['object']:<15} (score: {obj['score']:.2f}, freq: {obj['frequency']}, roles: {', '.join(obj['contexts'])})")

        if compound_objects:
            print("\nüîó OGGETTI COMPOSTI:")
            print("-" * 40)
            for i, (compound, score) in enumerate(compound_objects[:5], 1):
                print(f"{i:2d}. {compound} (score: {score:.2f})")

        if relationships:
            print("\nüîÑ RELAZIONI TROVATE:")
            print("-" * 40)
            for obj, rels in list(relationships.items())[:5]:
                if rels:
                    print(f"‚Ä¢ {obj}: {', '.join(rels[:2])}")

    def get_objects_for_trends(self, text, max_objects=5):
        """Ottiene oggetti ottimizzati per Google Trends"""

        print("üéØ ESTRAZIONE OGGETTI PER GOOGLE TRENDS")
        print("=" * 50)

        # Estrazione completa
        simple_objects, compound_objects, _ = self.extract_comprehensive(text, show_analysis=False)

        # Combina oggetti semplici e composti
        all_objects = []

        # Aggiungi oggetti semplici
        for obj in simple_objects:
            all_objects.append((obj['object'], obj['score'], 'simple'))

        # Aggiungi oggetti composti con bonus
        for compound, score in compound_objects:
            if len(compound.split()) <= 3:  # Max 3 parole per Google Trends
                all_objects.append((compound, score + 0.5, 'compound'))  # Bonus per compounds

        # Ordina per score e prendi i migliori
        all_objects.sort(key=lambda x: x[1], reverse=True)

        # Filtra per Google Trends
        trends_objects = []
        for obj, score, obj_type in all_objects:
            if (len(obj) >= 3 and len(obj) <= 30 and  # Lunghezza ragionevole
                len(obj.split()) <= 3 and  # Max 3 parole
                score > 1.0):  # Score minimo
                trends_objects.append(obj)

        final_objects = trends_objects[:max_objects]

        print(f"\n‚úÖ OGGETTI SELEZIONATI PER GOOGLE TRENDS:")
        for i, obj in enumerate(final_objects, 1):
            print(f"{i}. {obj}")

        return final_objects

# Inizializza l'estrattore
extractor = SmartObjectExtractor()

# ESEMPI DI UTILIZZO
print("\nüöÄ ESEMPI DI UTILIZZO:")
print("=" * 50)

# Esempio 1: Testo tecnologico
testo_tech = """
Apple ha lanciato il nuovo iPhone con fotocamera avanzata e processore pi√π veloce.
Le prestazioni del dispositivo superano quelle degli smartphone Android concorrenti.
La batteria dura tutto il giorno e il display OLED offre colori brillanti.
Gli sviluppatori possono creare app innovative per il nuovo sistema operativo iOS.
"""

print("1Ô∏è‚É£ ESEMPIO: Testo tecnologico")
objects_tech, compounds_tech, relations_tech = extractor.extract_comprehensive(testo_tech)

# INTERFACCIA INTERATTIVA
print("\n" + "="*60)
print("üéÆ INTERFACCIA INTERATTIVA")
print("="*60)

def analisi_oggetti_intelligente():
    """Funzione per l'analisi interattiva degli oggetti"""

    print("\nüìù Inserisci il testo da analizzare:")
    testo = input("Testo: ").strip()

    if not testo:
        print("‚ùå Testo non valido.")
        return

    print("\nOpzioni di analisi:")
    print("1. Analisi completa con visualizzazioni")
    print("2. Solo oggetti per Google Trends")
    print("3. Mostra albero delle dipendenze")

    scelta = input("Scelta (1-3): ").strip()

    if scelta == '1':
        objects, compounds, relationships = extractor.extract_comprehensive(testo)
        return objects
    elif scelta == '2':
        trends_objects = extractor.get_objects_for_trends(testo)
        return trends_objects
    elif scelta == '3':
        language = extractor.detect_language(testo)
        extractor.visualize_dependency_tree(testo, language)
        return []
    else:
        print("‚ùå Scelta non valida")
        return []

# INTEGRAZIONE CON GOOGLE TRENDS
def analisi_oggetti_trends():
    """Combina estrazione oggetti + analisi Google Trends"""

    print("üî• ANALISI COMPLETA: OGGETTI + GOOGLE TRENDS")
    print("=" * 60)

    # Input testo
    testo = input("üìù Inserisci il testo da analizzare: ").strip()

    if not testo:
        print("‚ùå Testo non valido.")
        return

    # Estrai oggetti
    print("\nüéØ STEP 1: Estrazione oggetti intelligente...")
    objects = extractor.get_objects_for_trends(testo, max_objects=3)

    if not objects:
        print("‚ùå Nessun oggetto trovato.")
        return

    # Analizza con Google Trends (se disponibile)
    print("\nüìä STEP 2: Analisi Google Trends...")
    try:
        # Verifica se l'analyzer √® disponibile (dalla prima cella)
        if 'analyzer' in globals():
            for obj in objects:
                print(f"\nüîç Analizzando: {obj}")
                analyzer.analizza_trend(obj, periodo='today 3-m', mostra_regioni=False)
        else:
            print("‚ö†Ô∏è Analyzer Google Trends non disponibile")
            print("üí° Esegui prima la cella con l'analyzer Google Trends!")

    except Exception as e:
        print(f"‚ùå Errore nell'analisi trends: {e}")

    print(f"\n‚úÖ ANALISI COMPLETATA!")
    print(f"üéØ Oggetti analizzati: {', '.join(objects)}")

print("\n‚úÖ ESTRATTORE INTELLIGENTE OGGETTI PRONTO!")
print("=" * 50)
print("üí° Funzioni disponibili:")
print("‚Ä¢ analisi_oggetti_intelligente() - Analisi interattiva")
print("‚Ä¢ analisi_oggetti_trends() - Oggetti + Google Trends")
print("‚Ä¢ extractor.extract_comprehensive(testo) - Estrazione completa")
print("‚Ä¢ extractor.get_objects_for_trends(testo) - Oggetti per trends")
print("‚Ä¢ extractor.visualize_dependency_tree(testo) - Mostra albero dipendenze")

print("\nüß† CARATTERISTICHE INTELLIGENTI:")
print("‚Ä¢ ‚úÖ Dependency parsing per identificare ruoli sintattici")
print("‚Ä¢ ‚úÖ Analisi semantica con WordNet per importanza concetti")
print("‚Ä¢ ‚úÖ Scoring automatico basato su struttura sintattica")
print("‚Ä¢ ‚úÖ Filtraggio intelligente senza liste predefinite")
print("‚Ä¢ ‚úÖ Estrazione oggetti composti con relazioni")
print("‚Ä¢ ‚úÖ Visualizzazione albero delle dipendenze per debug")

print("\nüéØ COME FUNZIONA L'INTELLIGENZA:")
print("=" * 40)
print("1. üå≥ DEPENDENCY PARSING:")
print("   - Identifica soggetti, oggetti diretti, complementi")
print("   - Analizza modificatori e composti")
print("   - Valuta posizione e ruolo sintattico")

print("\n2. üß† ANALISI SEMANTICA:")
print("   - Usa WordNet per concretezza/astrattezza")
print("   - Calcola specificit√† tramite iperonymi/iponimi")
print("   - Valuta importanza tramite definizioni ed esempi")

print("\n3. üìä SCORING COMBINATO:")
print("   - Score sintattico (ruolo nella frase)")
print("   - Score semantico (importanza concettuale)")
print("   - Score di contesto (entit√† nominate)")
print("   - Score di frequenza (ripetizioni)")

print("\n4. üîó RELAZIONI INTELLIGENTI:")
print("   - Identifica verbo-soggetto-oggetto")
print("   - Trova oggetti composti automaticamente")
print("   - Analizza catene di dipendenze")

print("\nüöÄ ESEMPI AVANZATI:")
print("=" * 30)

# Esempio complesso per mostrare le capacit√†
testo_complesso = """
La nuova intelligenza artificiale di Google utilizza algoritmi di machine learning
per migliorare la ricerca vocale. Il sistema neurali elabora il linguaggio naturale
e fornisce risposte precise agli utenti. Le aziende tecnologiche investono miliardi
in questa rivoluzione digitale che trasformer√† il mercato dei motori di ricerca.
"""

print("üìù ESEMPIO COMPLESSO:")
print(f"Testo: {testo_complesso[:100]}...")
print("\nüîç Eseguendo analisi...")

# Analisi rapida per esempio
try:
    objects_esempio = extractor.get_objects_for_trends(testo_complesso, max_objects=4)
    print(f"\nüí° Oggetti estratti automaticamente: {', '.join(objects_esempio)}")
except Exception as e:
    print(f"‚ö†Ô∏è Esempio non eseguibile: {e}")

print("\nüé≤ PROVA ANCHE TU:")
print("‚Ä¢ Copia un articolo di giornale")
print("‚Ä¢ Incolla una descrizione di prodotto")
print("‚Ä¢ Inserisci un post sui social")
print("‚Ä¢ Scrivi un testo tecnico")
print("\nL'estrattore identificher√† automaticamente gli oggetti pi√π rilevanti!")

print("\nüî¨ DEBUG E ANALISI:")
print("=" * 25)
print("Per capire come funziona l'algoritmo:")
print("‚Ä¢ extractor.visualize_dependency_tree(testo)")
print("‚Ä¢ extractor.extract_comprehensive(testo, show_analysis=True)")
print("‚Ä¢ Osserva i punteggi sintattici vs semantici nei grafici")

print("\n‚ö° OTTIMIZZAZIONI INCLUSE:")
print("‚Ä¢ üéØ Filtraggio automatico parole generiche")
print("‚Ä¢ üèÜ Priorit√† a soggetti e oggetti diretti")
print("‚Ä¢ üîç Bonus per entit√† nominate")
print("‚Ä¢ üìè Lunghezza ottimale per Google Trends")
print("‚Ä¢ üåê Supporto multilingua (IT/EN)")
print("‚Ä¢ üßÆ Normalizzazione e lemmatizzazione")

print("\n" + "="*60)
print("üéâ TUTTO PRONTO! Inizia con: analisi_oggetti_intelligente()")
print("="*60)

analisi_oggetti_intelligente()

# Analizzatore Integrato: Testo ‚Üí Oggetti ‚Üí Date ‚Üí Google Trends
# Pipeline completa per analisi automatica dei trend da testo libero

# Installazione librerie aggiuntive per date
!pip install dateparser python-dateutil

import re
import dateparser
from datetime import datetime, timedelta
from dateutil import parser as date_parser
import pandas as pd
import matplotlib.pyplot as plt
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# Estrattore Intelligente di Oggetti con Dependency Parsing e Analisi Semantica
# Identifica automaticamente gli oggetti rilevanti usando struttura sintattica e semantica

# Installazione delle librerie necessarie
!pip install nltk spacy textblob
!python -m spacy download it_core_news_sm
!python -m spacy download en_core_web_sm

import nltk
import spacy
from textblob import TextBlob
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter, defaultdict
import re
from nltk.corpus import wordnet as wn
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import WordNetLemmatizer
import warnings
warnings.filterwarnings('ignore')

# Download dei dati NLTK necessari
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('omw-1.4')

class SmartObjectExtractor:
    def __init__(self):
        """Inizializza l'estrattore intelligente di oggetti"""
        print("üß† Inizializzazione Estrattore Intelligente di Oggetti...")

        # Carica modelli di lingua
        try:
            self.nlp_it = spacy.load("it_core_news_sm")
            print("‚úÖ Modello italiano caricato")
        except:
            print("‚ö†Ô∏è Modello italiano non disponibile")
            self.nlp_it = None

        try:
            self.nlp_en = spacy.load("en_core_web_sm")
            print("‚úÖ Modello inglese caricato")
        except:
            print("‚ö†Ô∏è Modello inglese non disponibile")
            self.nlp_en = None

        # Inizializza lemmatizer per WordNet
        self.lemmatizer = WordNetLemmatizer()

        # Solo stopwords base (il resto lo determiniamo semanticamente)
        self.stop_words_it = set(stopwords.words('italian'))
        self.stop_words_en = set(stopwords.words('english'))

        # Dependency labels che indicano oggetti importanti
        self.important_deps = {
            'nsubj',      # soggetto nominale
            'dobj',       # oggetto diretto
            'pobj',       # oggetto di preposizione
            'nsubjpass',  # soggetto passivo
            'compound',   # sostantivo composto
            'appos',      # apposizione
            'conj'        # congiunzione (oggetti in lista)
        }

        # Dependency labels che indicano parole meno rilevanti
        self.weak_deps = {
            'det',        # determinanti
            'aux',        # ausiliari
            'cop',        # copula
            'mark',       # marcatori subordinanti
            'punct',      # punteggiatura
            'cc'          # congiunzioni coordinanti
        }

        print("‚úÖ Estrattore intelligente inizializzato!")

    def detect_language(self, text):
        """Rileva la lingua del testo"""
        try:
            blob = TextBlob(text)
            lang = blob.detect_language()
            return 'it' if lang in ['it', 'la'] else 'en'
        except:
            # Fallback basato su parole italiane comuni
            italian_indicators = ['il', 'la', 'di', 'che', 'e', 'a', 'un', 'per', 'con', 'da', 'in', 'del', 'alla', '√®']
            words = text.lower().split()
            italian_count = sum(1 for word in words if word in italian_indicators)
            return 'it' if italian_count > len(words) * 0.08 else 'en'

    def analyze_semantic_importance(self, token, language='it'):
        """Analizza l'importanza semantica di un token usando WordNet"""

        if not token.text.isalpha() or len(token.text) < 3:
            return 0.0

        # Cerca in WordNet
        synsets = wn.synsets(token.lemma_.lower())

        if not synsets:
            return 0.5  # Score neutro se non trovato

        importance_score = 0.0

        for synset in synsets:
            # 1. Concretezza: sostantivi concreti sono pi√π importanti
            if synset.pos() == 'n':  # sostantivo
                # Cerca iponimi (pi√π specifico = pi√π concreto)
                hyponyms = synset.hyponyms()
                if hyponyms:
                    importance_score += 0.3 * len(hyponyms[:5])  # Max bonus da iponimi

                # Cerca iperonymi (concetti generali sono meno importanti)
                hypernyms = synset.hypernyms()
                if len(hypernyms) > 3:  # Troppo astratto
                    importance_score -= 0.2

                # 2. Specificit√†: definizioni lunghe = concetti pi√π specifici
                definition_length = len(synset.definition().split())
                if definition_length > 10:
                    importance_score += 0.4
                elif definition_length < 5:
                    importance_score -= 0.2

                # 3. Esempi: presenza di esempi indica concretezza
                if synset.examples():
                    importance_score += 0.3

        return max(0.0, min(2.0, importance_score))  # Normalizza tra 0 e 2

    def analyze_syntactic_role(self, token):
        """Analizza il ruolo sintattico del token nell'albero delle dipendenze"""

        role_score = 0.0

        # 1. Dependency label principale
        if token.dep_ in self.important_deps:
            role_score += 2.0
        elif token.dep_ in self.weak_deps:
            role_score -= 1.0
        else:
            role_score += 0.5  # Score neutro per altri dep

        # 2. Posizione nella frase (inizio = pi√π importante)
        sentence_position = token.i / len(token.doc)
        if sentence_position < 0.3:  # Prime 30% della frase
            role_score += 0.5
        elif sentence_position > 0.8:  # Ultime 20% della frase
            role_score -= 0.3

        # 3. Presenza di modificatori (indica importanza)
        modifiers = list(token.children)
        if modifiers:
            # Aggettivi, avverbi, altre specifiche
            modifier_bonus = len([child for child in modifiers if child.pos_ in ['ADJ', 'ADV']]) * 0.3
            role_score += modifier_bonus

        # 4. Dipendenza dal verbo principale
        if token.head.pos_ == 'VERB' and token.dep_ in ['nsubj', 'dobj']:
            role_score += 1.0  # Soggetto/oggetto di verbo principale

        # 5. Lunghezza token (troppo corto spesso non significativo)
        if len(token.text) < 3:
            role_score -= 0.5
        elif len(token.text) > 8:
            role_score += 0.3  # Parole lunghe spesso pi√π specifiche

        return role_score

    def extract_objects_with_context(self, text, language='it', top_k=15):
        """Estrae oggetti usando dependency parsing e analisi semantica"""

        print(f"üß† Analisi intelligente oggetti (lingua: {language})")

        # Seleziona modello
        nlp = self.nlp_it if language == 'it' else self.nlp_en

        if not nlp:
            print("‚ùå Modello spaCy non disponibile")
            return []

        # Processa il testo
        doc = nlp(text)

        object_candidates = []

        # Analizza ogni token
        for token in doc:
            if token.pos_ in ['NOUN', 'PROPN'] and not token.is_stop:

                # 1. Score sintattico (dependency parsing)
                syntactic_score = self.analyze_syntactic_role(token)

                # 2. Score semantico (WordNet)
                semantic_score = self.analyze_semantic_importance(token, language)

                # 3. Score di contesto (entit√† nominate)
                context_score = 0.0
                for ent in doc.ents:
                    if token.text in ent.text:
                        context_score += 1.0  # Parte di entit√† nominata
                        break

                # 4. Score di frequenza nella frase
                frequency_score = len([t for t in doc if t.lemma_ == token.lemma_]) * 0.2

                # Score finale combinato
                final_score = syntactic_score + semantic_score + context_score + frequency_score

                object_candidates.append({
                    'text': token.lemma_.lower(),
                    'original': token.text,
                    'pos': token.pos_,
                    'dep': token.dep_,
                    'syntactic_score': syntactic_score,
                    'semantic_score': semantic_score,
                    'context_score': context_score,
                    'frequency_score': frequency_score,
                    'final_score': final_score,
                    'sentence_index': token.sent.start
                })

        # Raggruppa oggetti simili e somma scores
        grouped_objects = defaultdict(lambda: {
            'total_score': 0.0,
            'count': 0,
            'details': [],
            'best_form': ''
        })

        for obj in object_candidates:
            key = obj['text']
            grouped_objects[key]['total_score'] += obj['final_score']
            grouped_objects[key]['count'] += 1
            grouped_objects[key]['details'].append(obj)

            # Mantieni la forma migliore (pi√π frequente o con score pi√π alto)
            if not grouped_objects[key]['best_form'] or obj['final_score'] > max(d['final_score'] for d in grouped_objects[key]['details'][:-1]):
                grouped_objects[key]['best_form'] = obj['original']

        # Calcola score medio e applica bonus per frequenza
        final_objects = []
        for text, data in grouped_objects.items():
            avg_score = data['total_score'] / data['count']
            frequency_bonus = min(data['count'] * 0.3, 1.0)  # Max bonus 1.0

            final_score = avg_score + frequency_bonus

            final_objects.append({
                'object': data['best_form'],
                'lemma': text,
                'score': final_score,
                'frequency': data['count'],
                'avg_syntactic': sum(d['syntactic_score'] for d in data['details']) / len(data['details']),
                'avg_semantic': sum(d['semantic_score'] for d in data['details']) / len(data['details']),
                'contexts': list(set(d['dep'] for d in data['details']))
            })

        # Ordina per score finale
        final_objects.sort(key=lambda x: x['score'], reverse=True)

        return final_objects[:top_k]

    def extract_compound_objects(self, text, language='it'):
        """Estrae oggetti composti usando dependency parsing"""

        print("üîó Estrazione oggetti composti...")

        nlp = self.nlp_it if language == 'it' else self.nlp_en

        if not nlp:
            return []

        doc = nlp(text)
        compounds = []

        for token in doc:
            if token.pos_ in ['NOUN', 'PROPN']:
                compound_parts = [token.text.lower()]

                # Cerca modificatori che formano compounds
                for child in token.children:
                    if child.dep_ in ['compound', 'amod'] and child.pos_ in ['NOUN', 'PROPN', 'ADJ']:
                        compound_parts.insert(0, child.text.lower())
                    elif child.dep_ == 'prep':
                        # Gestisce costruzioni come "carta di credito"
                        for prep_child in child.children:
                            if prep_child.dep_ == 'pobj':
                                compound_phrase = f"{token.text.lower()} {child.text} {prep_child.text.lower()}"

                                # Valuta se √® un compound semanticamente significativo
                                compound_score = 0.0

                                # Controlla in WordNet se il compound esiste
                                compound_synsets = wn.synsets(compound_phrase.replace(' ', '_'))
                                if compound_synsets:
                                    compound_score += 2.0

                                # Controlla parti individuali
                                part_scores = []
                                for part in [token.text.lower(), prep_child.text.lower()]:
                                    part_synsets = wn.synsets(part)
                                    if part_synsets:
                                        part_scores.append(len(part_synsets))

                                if part_scores:
                                    compound_score += sum(part_scores) * 0.1

                                if compound_score > 0.5:
                                    compounds.append((compound_phrase, compound_score))

                # Crea compound da modificatori adiacenti
                if len(compound_parts) > 1:
                    compound_text = ' '.join(compound_parts)

                    # Valuta compound
                    compound_score = len(compound_parts) * 0.5

                    # Bonus se trovato in WordNet
                    compound_synsets = wn.synsets(compound_text.replace(' ', '_'))
                    if compound_synsets:
                        compound_score += 1.0

                    compounds.append((compound_text, compound_score))

        # Rimuovi duplicati e ordina per score
        unique_compounds = {}
        for compound, score in compounds:
            if compound not in unique_compounds or score > unique_compounds[compound]:
                unique_compounds[compound] = score

        sorted_compounds = sorted(unique_compounds.items(), key=lambda x: x[1], reverse=True)

        return sorted_compounds[:10]

    def analyze_object_relationships(self, text, language='it'):
        """Analizza le relazioni tra oggetti nel testo"""

        print("üîç Analisi relazioni tra oggetti...")

        nlp = self.nlp_it if language == 'it' else self.nlp_en

        if not nlp:
            return {}

        doc = nlp(text)
        relationships = defaultdict(list)

        # Trova relazioni verbo-oggetto
        for token in doc:
            if token.pos_ == 'VERB':
                subjects = []
                objects = []

                for child in token.children:
                    if child.dep_ in ['nsubj', 'nsubjpass'] and child.pos_ in ['NOUN', 'PROPN']:
                        subjects.append(child.lemma_.lower())
                    elif child.dep_ in ['dobj', 'pobj'] and child.pos_ in ['NOUN', 'PROPN']:
                        objects.append(child.lemma_.lower())

                # Registra relazioni
                verb = token.lemma_.lower()
                for subj in subjects:
                    for obj in objects:
                        relationships[subj].append(f"{verb} ‚Üí {obj}")
                        relationships[obj].append(f"‚Üê {verb} {subj}")

        return dict(relationships)

    def visualize_dependency_tree(self, text, language='it'):
        """Visualizza l'albero delle dipendenze per debug"""

        nlp = self.nlp_it if language == 'it' else self.nlp_en

        if not nlp:
            print("‚ùå Modello spaCy non disponibile")
            return

        doc = nlp(text)

        print("üå≥ ALBERO DELLE DIPENDENZE:")
        print("=" * 50)

        for token in doc:
            if token.pos_ in ['NOUN', 'PROPN', 'VERB']:
                indent = "  " * (token.depth if hasattr(token, 'depth') else 0)
                children_info = [f"{child.text}({child.dep_})" for child in token.children]
                children_str = f" ‚Üí [{', '.join(children_info)}]" if children_info else ""

                print(f"{indent}{token.text} ({token.pos_}, {token.dep_}){children_str}")

    def extract_comprehensive(self, text, top_k=10, show_analysis=True):
        """Estrazione completa con analisi dettagliata"""

        print("üéØ ESTRAZIONE COMPLETA OGGETTI")
        print("=" * 50)
        print(f"üìù Testo: {text[:100]}{'...' if len(text) > 100 else ''}")

        # Rileva lingua
        language = self.detect_language(text)
        print(f"üåç Lingua rilevata: {'Italiano' if language == 'it' else 'Inglese'}")
        print("-" * 50)

        # Mostra albero dipendenze se richiesto
        if show_analysis:
            self.visualize_dependency_tree(text, language)
            print()

        # Estrai oggetti semplici
        simple_objects = self.extract_objects_with_context(text, language, top_k)

        # Estrai oggetti composti
        compound_objects = self.extract_compound_objects(text, language)

        # Analizza relazioni
        relationships = self.analyze_object_relationships(text, language)

        # Visualizza risultati
        self.visualize_results(simple_objects, compound_objects, relationships, text)

        return simple_objects, compound_objects, relationships

    def visualize_results(self, simple_objects, compound_objects, relationships, text):
        """Visualizza i risultati dell'estrazione"""

        # Configura subplot
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Analisi Intelligente degli Oggetti', fontsize=16, fontweight='bold')

        # 1. Oggetti semplici - scores
        if simple_objects:
            objects = [obj['object'] for obj in simple_objects[:10]]
            scores = [obj['score'] for obj in simple_objects[:10]]

            bars = axes[0, 0].bar(range(len(objects)), scores, color='skyblue')
            axes[0, 0].set_title('Top Oggetti per Score', fontweight='bold')
            axes[0, 0].set_xlabel('Oggetti')
            axes[0, 0].set_ylabel('Score')
            axes[0, 0].set_xticks(range(len(objects)))
            axes[0, 0].set_xticklabels(objects, rotation=45, ha='right')

            # Aggiungi valori sulle barre
            for bar, score in zip(bars, scores):
                axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                               f'{score:.2f}', ha='center', va='bottom', fontsize=9)

        # 2. Breakdown scores (sintattico vs semantico)
        if simple_objects:
            syntactic_scores = [obj['avg_syntactic'] for obj in simple_objects[:8]]
            semantic_scores = [obj['avg_semantic'] for obj in simple_objects[:8]]
            labels = [obj['object'][:8] for obj in simple_objects[:8]]

            x = range(len(labels))
            width = 0.35

            axes[0, 1].bar([i - width/2 for i in x], syntactic_scores, width, label='Sintattico', color='orange')
            axes[0, 1].bar([i + width/2 for i in x], semantic_scores, width, label='Semantico', color='green')

            axes[0, 1].set_title('Score Sintattico vs Semantico', fontweight='bold')
            axes[0, 1].set_xlabel('Oggetti')
            axes[0, 1].set_ylabel('Score')
            axes[0, 1].set_xticks(x)
            axes[0, 1].set_xticklabels(labels, rotation=45, ha='right')
            axes[0, 1].legend()

        # 3. Frequenza oggetti
        if simple_objects:
            frequencies = [obj['frequency'] for obj in simple_objects[:10]]
            axes[0, 2].hist(frequencies, bins=5, alpha=0.7, color='lightgreen', edgecolor='black')
            axes[0, 2].set_title('Distribuzione Frequenze', fontweight='bold')
            axes[0, 2].set_xlabel('Frequenza nel testo')
            axes[0, 2].set_ylabel('Numero oggetti')

        # 4. Oggetti composti
        if compound_objects:
            comp_names = [comp[:15] for comp, _ in compound_objects[:8]]
            comp_scores = [score for _, score in compound_objects[:8]]

            axes[1, 0].bar(range(len(comp_names)), comp_scores, color='coral')
            axes[1, 0].set_title('Oggetti Composti', fontweight='bold')
            axes[1, 0].set_xlabel('Oggetti Composti')
            axes[1, 0].set_ylabel('Score')
            axes[1, 0].set_xticks(range(len(comp_names)))
            axes[1, 0].set_xticklabels(comp_names, rotation=45, ha='right')

        # 5. Dependency labels distribution
        if simple_objects:
            all_contexts = []
            for obj in simple_objects:
                all_contexts.extend(obj['contexts'])

            context_counts = Counter(all_contexts)

            if context_counts:
                contexts = list(context_counts.keys())[:6]
                counts = [context_counts[ctx] for ctx in contexts]

                axes[1, 1].pie(counts, labels=contexts, autopct='%1.1f%%')
                axes[1, 1].set_title('Ruoli Sintattici', fontweight='bold')

        # 6. Statistiche e top oggetti
        word_count = len(text.split())
        unique_objects = len(set(obj['lemma'] for obj in simple_objects))

        stats_text = f"""
        üìä STATISTICHE:
        ‚Ä¢ Parole totali: {word_count}
        ‚Ä¢ Oggetti unici: {unique_objects}
        ‚Ä¢ Oggetti composti: {len(compound_objects)}
        ‚Ä¢ Con relazioni: {len(relationships)}

        üèÜ TOP 5 OGGETTI:
        """

        for i, obj in enumerate(simple_objects[:5], 1):
            stats_text += f"\n{i}. {obj['object']} ({obj['score']:.2f})"

        axes[1, 2].text(0.1, 0.9, stats_text, transform=axes[1, 2].transAxes,
                        verticalalignment='top', fontsize=10,
                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        axes[1, 2].set_title('Statistiche', fontweight='bold')
        axes[1, 2].axis('off')

        plt.tight_layout()
        plt.show()

        # Stampa risultati testuali dettagliati
        print("\nüéØ OGGETTI PRINCIPALI:")
        print("=" * 60)

        for i, obj in enumerate(simple_objects[:8], 1):
            print(f"{i:2d}. {obj['object']:<15} (score: {obj['score']:.2f}, freq: {obj['frequency']}, roles: {', '.join(obj['contexts'])})")

        if compound_objects:
            print("\nüîó OGGETTI COMPOSTI:")
            print("-" * 40)
            for i, (compound, score) in enumerate(compound_objects[:5], 1):
                print(f"{i:2d}. {compound} (score: {score:.2f})")

        if relationships:
            print("\nüîÑ RELAZIONI TROVATE:")
            print("-" * 40)
            for obj, rels in list(relationships.items())[:5]:
                if rels:
                    print(f"‚Ä¢ {obj}: {', '.join(rels[:2])}")

    def get_objects_for_trends(self, text, max_objects=5):
        """Ottiene oggetti ottimizzati per Google Trends (ignora date nel testo)"""

        print("üéØ ESTRAZIONE OGGETTI PER GOOGLE TRENDS")
        print("=" * 50)
        print("üìÖ NOTA: Date nel testo vengono ignorate - si usa sempre l'ultimo anno")

        # Rimuovi riferimenti temporali dal testo prima dell'analisi
        cleaned_text = self.remove_temporal_references(text)

        # Estrazione completa sul testo pulito
        simple_objects, compound_objects, _ = self.extract_comprehensive(cleaned_text, show_analysis=False)

        # Combina oggetti semplici e composti
        all_objects = []

        # Aggiungi oggetti semplici
        for obj in simple_objects:
            all_objects.append((obj['object'], obj['score'], 'simple'))

        # Aggiungi oggetti composti con bonus
        for compound, score in compound_objects:
            if len(compound.split()) <= 3:  # Max 3 parole per Google Trends
                all_objects.append((compound, score + 0.5, 'compound'))  # Bonus per compounds

        # Ordina per score e prendi i migliori
        all_objects.sort(key=lambda x: x[1], reverse=True)

        # Filtra per Google Trends
        trends_objects = []
        for obj, score, obj_type in all_objects:
            if (len(obj) >= 3 and len(obj) <= 30 and  # Lunghezza ragionevole
                len(obj.split()) <= 3 and  # Max 3 parole
                score > 1.0):  # Score minimo
                trends_objects.append(obj)

        final_objects = trends_objects[:max_objects]

        print(f"\n‚úÖ OGGETTI SELEZIONATI PER GOOGLE TRENDS (senza date):")
        for i, obj in enumerate(final_objects, 1):
            print(f"{i}. {obj}")

        return final_objects

    def remove_temporal_references(self, text):
        """Rimuove riferimenti temporali dal testo per evitare interferenze"""

        # Pattern per date e riferimenti temporali da rimuovere
        temporal_patterns = [
            r'\b\d{1,2}[/\-\.]\d{1,2}[/\-\.]\d{4}\b',  # 15/03/2027
            r'\b\d{4}[/\-\.]\d{1,2}[/\-\.]\d{1,2}\b',  # 2027/03/15
            r'\b(gennaio|febbraio|marzo|aprile|maggio|giugno|luglio|agosto|settembre|ottobre|novembre|dicembre)\s+\d{4}\b',
            r'\b\d{4}\b',  # Anni singoli come 2027
            r'\bnel\s+\d{4}\b',
            r'\bentro\s+il\s+\d{4}\b',
            r'\bdal\s+\d{4}\b',
            r'\bnell?\s*(ultimo|prossimo|scorso)\s+(anno|mese|trimestre)\b',
            r'\b(ieri|oggi|domani|dopodomani)\b',
            r'\b(la\s+)?(settimana|mese)\s+(scorsa|prossima|passata)\b',
        ]

        cleaned_text = text

        for pattern in temporal_patterns:
            cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.IGNORECASE)

        # Pulisci spazi multipli
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()

        print(f"üßπ Testo originale: {text[:50]}...")
        print(f"üßπ Testo pulito: {cleaned_text[:50]}...")

        return cleaned_text

# Inizializza l'estrattore
extractor = SmartObjectExtractor()

# ESEMPI DI UTILIZZO
print("\nüöÄ ESEMPI DI UTILIZZO:")
print("=" * 50)

# Esempio 1: Testo tecnologico
testo_tech = """
Apple ha lanciato il nuovo iPhone con fotocamera avanzata e processore pi√π veloce.
Le prestazioni del dispositivo superano quelle degli smartphone Android concorrenti.
La batteria dura tutto il giorno e il display OLED offre colori brillanti.
Gli sviluppatori possono creare app innovative per il nuovo sistema operativo iOS.
"""

print("1Ô∏è‚É£ ESEMPIO: Testo tecnologico")
objects_tech, compounds_tech, relations_tech = extractor.extract_comprehensive(testo_tech)

# INTERFACCIA INTERATTIVA
print("\n" + "="*60)
print("üéÆ INTERFACCIA INTERATTIVA")
print("="*60)

def analisi_oggetti_intelligente():
    """Funzione per l'analisi interattiva degli oggetti"""

    print("\nüìù Inserisci il testo da analizzare:")
    testo = input("Testo: ").strip()

    if not testo:
        print("‚ùå Testo non valido.")
        return

    print("\nOpzioni di analisi:")
    print("1. Analisi completa con visualizzazioni")
    print("2. Solo oggetti per Google Trends")
    print("3. Mostra albero delle dipendenze")

    scelta = input("Scelta (1-3): ").strip()

    if scelta == '1':
        objects, compounds, relationships = extractor.extract_comprehensive(testo)
        return objects
    elif scelta == '2':
        trends_objects = extractor.get_objects_for_trends(testo)
        return trends_objects
    elif scelta == '3':
        language = extractor.detect_language(testo)
        extractor.visualize_dependency_tree(testo, language)
        return []
    else:
        print("‚ùå Scelta non valida")
        return []

# INTEGRAZIONE CON GOOGLE TRENDS
def analisi_oggetti_trends():
    """Combina estrazione oggetti + analisi Google Trends"""

    print("üî• ANALISI COMPLETA: OGGETTI + GOOGLE TRENDS")
    print("=" * 60)

    # Input testo
    testo = input("üìù Inserisci il testo da analizzare: ").strip()

    if not testo:
        print("‚ùå Testo non valido.")
        return

    # Estrai oggetti
    print("\nüéØ STEP 1: Estrazione oggetti intelligente...")
    objects = extractor.get_objects_for_trends(testo, max_objects=3)

    if not objects:
        print("‚ùå Nessun oggetto trovato.")
        return

    # Analizza con Google Trends (se disponibile)
    print("\nüìä STEP 2: Analisi Google Trends...")
    try:
        # Verifica se l'analyzer √® disponibile (dalla prima cella)
        if 'analyzer' in globals():
            for obj in objects:
                print(f"\nüîç Analizzando: {obj}")
                analyzer.analizza_trend(obj, periodo='today 3-m', mostra_regioni=False)
        else:
            print("‚ö†Ô∏è Analyzer Google Trends non disponibile")
            print("üí° Esegui prima la cella con l'analyzer Google Trends!")

    except Exception as e:
        print(f"‚ùå Errore nell'analisi trends: {e}")

    print(f"\n‚úÖ ANALISI COMPLETATA!")
    print(f"üéØ Oggetti analizzati: {', '.join(objects)}")

print("\n‚úÖ ESTRATTORE INTELLIGENTE OGGETTI PRONTO!")
print("=" * 50)
print("üí° Funzioni disponibili:")
print("‚Ä¢ analisi_oggetti_intelligente() - Analisi interattiva")
print("‚Ä¢ analisi_oggetti_trends() - Oggetti + Google Trends")
print("‚Ä¢ extractor.extract_comprehensive(testo) - Estrazione completa")
print("‚Ä¢ extractor.get_objects_for_trends(testo) - Oggetti per trends")
print("‚Ä¢ extractor.visualize_dependency_tree(testo) - Mostra albero dipendenze")

print("\nüß† CARATTERISTICHE INTELLIGENTI:")
print("‚Ä¢ ‚úÖ Dependency parsing per identificare ruoli sintattici")
print("‚Ä¢ ‚úÖ Analisi semantica con WordNet per importanza concetti")
print("‚Ä¢ ‚úÖ Scoring automatico basato su struttura sintattica")
print("‚Ä¢ ‚úÖ Filtraggio intelligente senza liste predefinite")
print("‚Ä¢ ‚úÖ Estrazione oggetti composti con relazioni")
print("‚Ä¢ ‚úÖ Visualizzazione albero delle dipendenze per debug")

print("\nüéØ COME FUNZIONA L'INTELLIGENZA:")
print("=" * 40)
print("1. üå≥ DEPENDENCY PARSING:")
print("   - Identifica soggetti, oggetti diretti, complementi")
print("   - Analizza modificatori e composti")
print("   - Valuta posizione e ruolo sintattico")

print("\n2. üß† ANALISI SEMANTICA:")
print("   - Usa WordNet per concretezza/astrattezza")
print("   - Calcola specificit√† tramite iperonymi/iponimi")
print("   - Valuta importanza tramite definizioni ed esempi")

print("\n3. üìä SCORING COMBINATO:")
print("   - Score sintattico (ruolo nella frase)")
print("   - Score semantico (importanza concettuale)")
print("   - Score di contesto (entit√† nominate)")
print("   - Score di frequenza (ripetizioni)")

print("\n4. üîó RELAZIONI INTELLIGENTI:")
print("   - Identifica verbo-soggetto-oggetto")
print("   - Trova oggetti composti automaticamente")
print("   - Analizza catene di dipendenze")

print("\nüöÄ ESEMPI AVANZATI:")
print("=" * 30)

# Esempio complesso per mostrare le capacit√†
testo_complesso = """
La nuova intelligenza artificiale di Google utilizza algoritmi di machine learning
per migliorare la ricerca vocale. Il sistema neurali elabora il linguaggio naturale
e fornisce risposte precise agli utenti. Le aziende tecnologiche investono miliardi
in questa rivoluzione digitale che trasformer√† il mercato dei motori di ricerca.
"""

print("üìù ESEMPIO COMPLESSO:")
print(f"Testo: {testo_complesso[:100]}...")
print("\nüîç Eseguendo analisi...")

# Analisi rapida per esempio
try:
    objects_esempio = extractor.get_objects_for_trends(testo_complesso, max_objects=4)
    print(f"\nüí° Oggetti estratti automaticamente: {', '.join(objects_esempio)}")
except Exception as e:
    print(f"‚ö†Ô∏è Esempio non eseguibile: {e}")

print("\nüé≤ PROVA ANCHE TU:")
print("‚Ä¢ Copia un articolo di giornale")
print("‚Ä¢ Incolla una descrizione di prodotto")
print("‚Ä¢ Inserisci un post sui social")
print("‚Ä¢ Scrivi un testo tecnico")
print("\nL'estrattore identificher√† automaticamente gli oggetti pi√π rilevanti!")

print("\nüî¨ DEBUG E ANALISI:")
print("=" * 25)
print("Per capire come funziona l'algoritmo:")
print("‚Ä¢ extractor.visualize_dependency_tree(testo)")
print("‚Ä¢ extractor.extract_comprehensive(testo, show_analysis=True)")
print("‚Ä¢ Osserva i punteggi sintattici vs semantici nei grafici")

print("\n‚ö° OTTIMIZZAZIONI INCLUSE:")
print("‚Ä¢ üéØ Filtraggio automatico parole generiche")
print("‚Ä¢ üèÜ Priorit√† a soggetti e oggetti diretti")
print("‚Ä¢ üîç Bonus per entit√† nominate")
print("‚Ä¢ üìè Lunghezza ottimale per Google Trends")
print("‚Ä¢ üåê Supporto multilingua (IT/EN)")
print("‚Ä¢ üßÆ Normalizzazione e lemmatizzazione")

print("\n" + "="*60)
print("üéâ TUTTO PRONTO! Inizia con: analisi_oggetti_intelligente()")
print("="*60)

"""vorrei predirre il prezzo dei biscotti al cacao nel marzo 2027. sono fatti con uova, farina, latte e burro"""

analisi_da_testo()